import sglang as sgl
import json
import time
from chat.chat_prompt import *

# 1. å®šä¹‰ SGLang ç”Ÿæˆå‡½æ•°
# è¿™ä¸ªå‡½æ•°å°†ä½œä¸ºæˆ‘ä»¬çš„ prompt æ¨¡æ¿ã€‚SGLang ä¼šå°†å‡½æ•°å†…çš„é€»è¾‘ç¼–è¯‘æˆé«˜æ•ˆçš„è¯·æ±‚ã€‚
@sgl.function
def generate_with_prefix(s, system_template, user_template, common_prefix, user_prompt):
    """
    SGLang å‡½æ•°ï¼Œç”¨äºæ„å»ºå¹¶æ‰§è¡Œä¸€æ¬¡ç”Ÿæˆè¯·æ±‚ã€‚
    - s: SGLang å†…éƒ¨çŠ¶æ€å¯¹è±¡ï¼Œå¿…é¡»ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ã€‚
    - system_template/user_template: æ¨¡å‹çš„å¯¹è¯æ¨¡æ¿ã€‚
    - common_prefix: å…±äº«çš„å‰ç¼€å†…å®¹ã€‚
    - user_prompt: æ¯æ¬¡è¯·æ±‚ä¸åŒçš„ç”¨æˆ·æŒ‡ä»¤éƒ¨åˆ†ã€‚
    """
    s += system_template.format(system_instruction=common_prefix)
    s += user_template.format(user_instruction=user_prompt)
    s += sgl.gen("result") # "result" æ˜¯æˆ‘ä»¬ä¸ºç”Ÿæˆå†…å®¹æŒ‡å®šçš„å˜é‡å

def main():
    # 2. å¯åŠ¨ SGLang Runtime åç«¯
    # å‚æ•°ä¸ vLLM ç±»ä¼¼ï¼Œä½†åç§°å¯èƒ½ä¸åŒ (e.g., model -> model_path, tensor_parallel_size -> tp_size)
    runtime = sgl.Runtime(
        model_path="/home/hdd/model/Qwen2.5-32B-Instruct",
        tp_size=2,
        # SGLang ä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜ï¼Œé€šå¸¸ä¸éœ€è¦ gpu_memory_utilization
        # log_level="DEBUG" # å¯åœ¨è°ƒè¯•æ—¶å¼€å¯
    )
    sgl.set_default_backend(runtime)
    print("SGLang Runtime a-OK! ğŸš€")

    # 3. åŠ è½½æ•°æ® (ä¸åŸä»£ç ç›¸åŒ)
    try:
        with open(f"./logs/stage/Meta-Llama-3-8B-Instruct_evolve_basic_forward_evolve_shared_prefix_rgb_zyz_0.json", 'r', encoding='utf-8') as file:
            data_list = json.load(file)
    except FileNotFoundError:
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„JSONæ–‡ä»¶ã€‚å°†ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿæ•°æ®ç‚¹è¿›è¡Œæ¼”ç¤ºã€‚")
        data_list = [
            {
                'query': 'What is the capital of France?',
                'answer': 'Paris',
                'filtered_retrieve_result': ['Paris is the capital and most populous city of France.'],
                'response': 'The capital of France is Paris.',
                'label': True
            }
        ]

    forward_total_time = []
    feedback_total_time = []

    # 4. å¾ªç¯å¤„ç†æ•°æ®
    for index, data_item in enumerate(data_list):
        if index >= 1:
            break
        print(f"--- Processing item {index+1}/{len(data_list)} ---")
        query = data_item['query']
        response = data_item['response']
        filtered_retrieve_result = data_item['filtered_retrieve_result']
        
        context = ""
        for idx, sentence in enumerate(filtered_retrieve_result, start=0):
            context += f"Path {idx}:\t{sentence}\n"
        
        # å‡†å¤‡ prompts (ä¸åŸä»£ç ç›¸åŒ)
        prefix = shared_prefix.format(knowledge_sequences=context)
        query_prompt_user = chat_with_graphrag_for_response_user_shared_prefix.format(question=query)
        feedback_prompt_user = score_feedback_prompt_standard_user_shared_prefix.format(question=query, last_response=response)
        
        # ----- SGLang è°ƒç”¨æ–¹å¼ -----
        
        # 5. ç¬¬ä¸€æ¬¡è¯·æ±‚ (Forward)
        forward_start_time = time.perf_counter()
        state1 = generate_with_prefix.run(
            system_template=qwen2_instruct_system,
            user_template=qwen2_instruct_user,
            common_prefix=prefix,
            user_prompt=query_prompt_user,
            temperature=0.0,
            max_new_tokens=2048,
            stop=["<|im_end|>", "<|eot_id|>", "<|end_of_text|>", "<|endoftext|>"]
        )
        forward_end_time = time.perf_counter()

        # 6. ç¬¬äºŒæ¬¡è¯·æ±‚ (Feedback)
        # SGLang çš„ Runtime ä¼šè‡ªåŠ¨æ£€æµ‹åˆ° common_prefix éƒ¨åˆ†ä¸ä¸Šä¸€æ¬¡è¯·æ±‚ç›¸åŒï¼Œ
        # å¹¶ç›´æ¥å¤ç”¨å…¶ KV Cacheï¼Œæå¤§åœ°æå‡äº†ç¬¬äºŒæ¬¡è¯·æ±‚çš„é€Ÿåº¦ã€‚
        feedback_start_time = time.perf_counter()
        state2 = generate_with_prefix.run(
            system_template=qwen2_instruct_system,
            user_template=qwen2_instruct_user,
            common_prefix=prefix,
            user_prompt=feedback_prompt_user,
            temperature=0.0,
            max_new_tokens=2048,
            stop=["<|im_end|>", "<|eot_id|>", "<|end_of_text|>", "<|endoftext|>"]
        )
        feedback_end_time = time.perf_counter()
        
        forward_time = forward_end_time - forward_start_time
        feedback_time = feedback_end_time - feedback_start_time
        
        forward_total_time.append(forward_time)
        feedback_total_time.append(feedback_time)

        # 7. è·å–ç»“æœ
        # ç»“æœå­˜å‚¨åœ¨è¿”å›çš„ state å¯¹è±¡ä¸­ï¼Œé€šè¿‡ sgl.gen() ä¸­å®šä¹‰çš„å˜é‡å ("result") è®¿é—®ã€‚
        print(f"Query: {query}")
        print("Answer 1:", state1["result"])
        print("Answer 2:", state2["result"])
        print(f"Time - Forward: {forward_time:.4f}s, Feedback: {feedback_time:.4f}s")
        print("-" * 20)

    # è®¡ç®—å¹³å‡æ—¶é—´ (ä¸åŸä»£ç ç›¸åŒ)
    if data_list:
        avg_forward = sum(forward_total_time) / len(forward_total_time)
        avg_feedback = sum(feedback_total_time) / len(feedback_total_time)
        print("\n--- Average Times ---")
        print(f"Average Forward Time: {avg_forward:.4f} seconds")
        print(f"Average Feedback Time: {avg_feedback:.4f} seconds")
        
    # å…³é—­ runtime (å¯é€‰ï¼Œä½†åœ¨è„šæœ¬ç»“æŸæ—¶æ˜¯å¥½ä¹ æƒ¯)
    runtime.shutdown()

if __name__ == "__main__":
    main()

# python -m llmragenv.sglang_test