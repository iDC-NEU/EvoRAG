import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"  # å¿…é¡»åœ¨å¯¼å…¥ torch æˆ– vllm ä¹‹å‰è®¾ç½®ï¼
from vllm import LLM, SamplingParams
import json
import time
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from chat.chat_prompt import * # å‡è®¾æ‚¨çš„promptæ¨¡æ¿éƒ½åœ¨è¿™é‡Œ
import argparse


def parse_vllm_metrics(metrics):
    # ... (æ‚¨çš„ metrics è§£æå‡½æ•°ä¿æŒä¸å˜)
    print(f"metrics: {metrics}")
    arrival_time = metrics.arrival_time
    first_scheduled_time = metrics.first_scheduled_time
    first_token_time = metrics.first_token_time
    finished_time = metrics.finished_time
    last_token_time = metrics.last_token_time
    time_in_queue = metrics.time_in_queue

    ttft_time = first_token_time - first_scheduled_time
    decode_time = finished_time - first_token_time
    generate_time = finished_time - first_scheduled_time
    wait_scheduled_time = first_scheduled_time - arrival_time

    end_to_end = finished_time - arrival_time

    return end_to_end, generate_time, ttft_time, decode_time, wait_scheduled_time


def main_original_sequential():
    """ æ‚¨åŸå§‹çš„é€æ¡å¤„ç†å‡½æ•°ï¼Œä¿ç•™ç”¨äºå¯¹æ¯” """
    # ... (å°†æ‚¨åŸæ¥çš„ main å‡½æ•°ä»£ç ç²˜è´´åˆ°è¿™é‡Œ)
    pass


def main_batched(args):
    """ ä¼˜åŒ–åçš„æ‰¹é‡å¤„ç†ç‰ˆæœ¬ """
    print("ğŸš€ Running Batched Version for Maximum Throughput ğŸš€")
    
    model_path = "/home/hdd/model/Llama-3.1-8B-Instruct"
    # model_path = "/home/hdd/model/Qwen2.5-32B-Instruct"

    # 1. å¯åŠ¨ vLLM æ¨¡å‹ (ä¸åŸä»£ç ç›¸åŒ)
    llm = LLM(
        # model="/home/hdd/model/Qwen2.5-32B-Instruct",
        # model="/home/hdd/model/Llama-3.1-8B-Instruct",
        model=model_path,
        tensor_parallel_size=2,
        gpu_memory_utilization=0.95,
        enable_prefix_caching=True,
        swap_space=128,
        # max_model_len=8192,
        # use_v2_block_manager=True,      # å¯ç”¨ PagedAttention
        # enable_chunked_prefill=True,    # å¯ç”¨æµå¼é¢„å¡«å……
        # max_num_seqs=128,       # æ§åˆ¶å¹¶å‘åºåˆ—æ•°
        # block_size=16,          # é»˜è®¤ 16ï¼Œå¯å°è¯• 32 å‡å°‘ç¢ç‰‡
    )

    # 2. å®šä¹‰é‡‡æ ·å‚æ•° (ä¸åŸä»£ç ç›¸åŒ)
    params = SamplingParams(
        temperature=0.0, 
        max_tokens=256,
        stop=["<|im_end|>", "<|eot_id|>", "<|end_of_text|>", "<|endoftext|>"],
    )

    params_2 = SamplingParams(
        temperature=0.0, 
        max_tokens=80,
        stop=["<|im_end|>", "<|eot_id|>", "<|end_of_text|>", "<|endoftext|>"],
    )

    # 3. åŠ è½½æ•°æ® (ä¸åŸä»£ç ç›¸åŒ)
    try:
        with open(f"./logs/stage/Qwen2.5-32B-Instruct_evolve_basic_forward_evolve_shared_prefix_{args.dataset}_0.json", 'r', encoding='utf-8') as file:
            data_list = json.load(file)
            
        # if args.dataset == 'rgb':
        #     with open(f"/home/zhangyz/RAG_2025/logs/rgb/Meta-Llama-3-8B-Instruct_kg_modify_llama_reproduce_forword_rgb_0.json", 'r', encoding='utf-8') as file:
        #         data_list = json.load(file)
        # elif args.dataset == "multihop":
        #     with open(f"/home/zhangyz/RAG_2025/logs/multihop/Meta-Llama-3-8B-Instruct_kg_modify_llama_reproduce_forword_multihop_0.json", 'r', encoding='utf-8') as file:
        #         data_list = json.load(file)
        # elif args.dataset == "hotpotqa_1":
        #     with open(f"/home/zhangyz/RAG_2025/logs/hotpotqa600/Meta-Llama-3-8B-Instruct_kg_modify_llama_reproduce_forword_hotpotqa600_0.json", 'r', encoding='utf-8') as file:
        #         data_list = json.load(file)
        # else:
        #     with open(f"./logs/stage/Qwen2.5-32B-Instruct_evolve_basic_forward_evolve_shared_prefix_{args.dataset}_0.json", 'r', encoding='utf-8') as file:
        #         data_list = json.load(file)
            # data_list = data_list[0:1]
    except FileNotFoundError:
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„JSONæ–‡ä»¶ã€‚å°†ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿæ•°æ®ç‚¹è¿›è¡Œæ¼”ç¤ºã€‚")
        data_list = [
            {'query': 'What is the capital of France?', 'answer': 'Paris', 'filtered_retrieve_result': ['Paris is the capital and most populous city of France.'], 'response': 'The capital of France is Paris.', 'label': True},
            {'query': 'Who wrote "Hamlet"?', 'answer': 'William Shakespeare', 'filtered_retrieve_result': ['Hamlet is a tragedy written by William Shakespeare.'], 'response': 'William Shakespeare wrote "Hamlet".', 'label': True}
        ]


    # for batch_size in [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]:
    batch_size = args.batch_size
    print(f"\n\n===== Processing Batch Size: {batch_size} =====")
    print(f"================================================================================================================")
    print(f"================================================================================================================")
    print(f"================================================================================================================")
    data_list = data_list[0:batch_size]
    # --- é˜¶æ®µä¸€ï¼šå‡†å¤‡å¹¶æ‰§è¡Œæ‰€æœ‰ Forward è¯·æ±‚ ---

    print(f"--- Preparing {len(data_list)} forward prompts for batching... ---")
    all_forward_prompts = []
    for data_item in data_list:
        context = "".join([f"Path {idx}:\t{sentence}\n" for idx, sentence in enumerate(data_item['filtered_retrieve_result'])])
        prefix = shared_prefix.format(knowledge_sequences=context)
        query_prompt_user = chat_with_graphrag_for_response_user_shared_prefix.format(question=data_item['query'])
        
        # æ„å»ºå®Œæ•´çš„prompt
        if "Qwen2.5" in model_path:
            full_prompt = qwen2_instruct_system.format(system_instruction=prefix) + qwen2_instruct_user.format(user_instruction=query_prompt_user)
        elif "Llama" in model_path:
            full_prompt = llama_instruct_system.format(system_instruction=prefix) + llama_instruct_user.format(user_instruction=query_prompt_user)
        all_forward_prompts.append(full_prompt)

    # ååé‡
    print("--- Starting Stage 1: Generating all forward responses (batched)... ---")
    batch_forward_start_time = time.perf_counter()
    outputs1 = llm.generate(all_forward_prompts, params)
    batch_forward_end_time = time.perf_counter()
    print(f"--- Stage 1 finished in {batch_forward_end_time - batch_forward_start_time:.4f} seconds. ---")
    print(f"--- Average time per request: {(batch_forward_end_time - batch_forward_start_time) / len(data_list):.4f} seconds ---")
    print(f"answer : {outputs1[0].outputs[0].text}")
    print(f"prompt len: {len(outputs1[0].prompt_token_ids)}")
    print(f"token len: {len(outputs1[0].outputs[0].token_ids)}")
    print(f"num_cached_tokens: {outputs1[0].num_cached_tokens}")


    # --- é˜¶æ®µäºŒï¼šå‡†å¤‡å¹¶æ‰§è¡Œæ‰€æœ‰ Feedback è¯·æ±‚ ---

    print(f"--- Preparing {len(data_list)} feedback prompts for batching... ---")
    all_feedback_prompts = []
    # æ³¨æ„ï¼šæˆ‘ä»¬å¿…é¡»ä¿æŒåŸå§‹ data_list å’Œ outputs1 çš„é¡ºåºä¸€è‡´æ€§
    for data_item, forward_output in zip(data_list, outputs1):
        # ä»ç¬¬ä¸€æ¬¡è¯·æ±‚çš„ç»“æœä¸­è·å– response
        generated_response = forward_output.outputs[0].text

        context = "".join([f"Path {idx}:\t{sentence}\n" for idx, sentence in enumerate(data_item['filtered_retrieve_result'])])
        prefix = shared_prefix.format(knowledge_sequences=context)
        # feedback_prompt_user = score_feedback_prompt_standard_user_test_optimized.format(question=data_item['query'], last_response=generated_response)
        feedback_prompt_user = score_feedback_prompt_standard_user_shared_prefix.format(question=data_item['query'], last_response=generated_response)

        # æ„å»ºå®Œæ•´çš„prompt
        if "Qwen2.5" in model_path:
            full_prompt = qwen2_instruct_system.format(system_instruction=prefix) + qwen2_instruct_user.format(user_instruction=feedback_prompt_user)
        elif "Llama" in model_path:
            full_prompt = llama_instruct_system.format(system_instruction=prefix) + llama_instruct_user.format(user_instruction=feedback_prompt_user)
        all_feedback_prompts.append(full_prompt)

    print("--- Starting Stage 2: Generating all feedback scores (batched)... ---")
    batch_feedback_start_time = time.perf_counter()
    outputs2 = llm.generate(all_feedback_prompts, params_2)
    batch_feedback_end_time = time.perf_counter()
    print(f"--- Stage 2 finished in {batch_feedback_end_time - batch_feedback_start_time:.4f} seconds. ---")
    print(f"--- Average time per request: {(batch_feedback_end_time - batch_feedback_start_time) / len(data_list):.4f} seconds ---")
    print(f"answer : {outputs2[0].outputs[0].text}")
    print(f"prompt len: {len(outputs2[0].prompt_token_ids)}")
    print(f"token len: {len(outputs2[0].outputs[0].token_ids)}")
    print(f"num_cached_tokens: {outputs2[0].num_cached_tokens}")


    # --- 4. ç»“æœå¤„ç†ä¸æŒ‡æ ‡è®¡ç®— ---
    
    # åˆå§‹åŒ–æŒ‡æ ‡åˆ—è¡¨
    forward_metrics_totals = [[] for _ in range(5)]
    feedback_metrics_totals = [[] for _ in range(5)]


    metric_names = ["end2end", "generate", "prefill", "decode", "wait_scheduled"]
    print("\n--- Processing Batch Results ---")
    for i in range(len(data_list)):
        forward_output = outputs1[i]
        feedback_output = outputs2[i]
        
        # è§£æå¹¶ç´¯åŠ  Forward æŒ‡æ ‡
        metrics = parse_vllm_metrics(forward_output.metrics)
        for j in range(5):
            forward_metrics_totals[j].append(metrics[j])
            # print(f"{metric_names[j]}_time_forward_item_{i}: {metrics[j]:.4f} seconds")
            
        # è§£æå¹¶ç´¯åŠ  Feedback æŒ‡æ ‡
        metrics = parse_vllm_metrics(feedback_output.metrics)
        for j in range(5):
            feedback_metrics_totals[j].append(metrics[j])

        # (å¯é€‰) æ‰“å°å•é¡¹ç»“æœ
        # if i < min(5, len(data_list)): # åªæ‰“å°å‰5ä¸ªç¤ºä¾‹ç»“æœ
        #     print(f"\n--- Item {i+1} ---")
        #     print(f"Query: {data_list[i]['query']}")
        #     print("Answer 1:", forward_output.outputs[0].text)
        #     # print("feedback prompt:", all_feedback_prompts[i])
        #     print("Answer 2:", feedback_output.outputs[0].text)
        #     print(outputs1[i])
        #     print("-" * 20)

    # è®¡ç®—å¹¶æ‰“å°å¹³å‡æŒ‡æ ‡
    if data_list:
        metric_names = ["end2end", "generate", "prefill", "decode", "wait_scheduled"]
        
        print("\n--- Forward Average Times (Batched) ---")
        for name, totals in zip(metric_names, forward_metrics_totals):
            avg_time = sum(totals) / len(totals)
            print(f"  Average {name}_time: {avg_time:.4f} seconds")

        print("\n--- Feedback Average Times (Batched) ---")
        for name, totals in zip(metric_names, feedback_metrics_totals):
            avg_time = sum(totals) / len(totals)
            print(f"  Average {name}_time: {avg_time:.4f} seconds")


if __name__ == "__main__":
    # è¿è¡Œä¼˜åŒ–åçš„æ‰¹é‡å¤„ç†ç‰ˆæœ¬

    parser = argparse.ArgumentParser(description="LLMRag Workload")
    parser.add_argument("--dataset", type=str, help="dataset", default='rgb_zyz')
    parser.add_argument("--batch_size", type=int, help="batch_size", default=5)
    
    args = parser.parse_args()

    main_batched(args)
    
    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è¿è¡ŒåŸå§‹ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
    # main_original_sequential()

    # python -m llmragenv.vllm_batch > ./logs/test/vllm_batch.log 2>&1


# 10

# --- Forward Average Times (Batched) ---
#   Average end2end_time: 9.5421 seconds
#   Average generate_time: 9.4892 seconds
#   Average prefill_time: 8.7937 seconds
#   Average decode_time: 0.6955 seconds
#   Average wait_scheduled_time: 0.0529 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 4.9838 seconds
#   Average generate_time: 4.9415 seconds
#   Average prefill_time: 2.3816 seconds
#   Average decode_time: 2.5599 seconds
#   Average wait_scheduled_time: 0.0423 seconds

# 30

# --- Forward Average Times (Batched) ---
#   Average end2end_time: 28.9216 seconds
#   Average generate_time: 20.1355 seconds
#   Average prefill_time: 10.7428 seconds
#   Average decode_time: 9.3927 seconds
#   Average wait_scheduled_time: 8.7861 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 14.5462 seconds
#   Average generate_time: 11.4905 seconds
#   Average prefill_time: 2.7282 seconds
#   Average decode_time: 8.7623 seconds
#   Average wait_scheduled_time: 3.0557 seconds

# åˆ†æ˜æ˜¯ä¸€ä¸ªçº¿æ€§çš„æ—¶é—´

# æ‰¹å¤„ç†æå‡çš„æ˜¯ååé‡


# --- Stage 1 finished in 307.4585 seconds. ---

# --- Stage 2 finished in 424.2424 seconds. ---

# --- Forward Average Times (Batched) ---
#   Average end2end_time: 184.0141 seconds
#   Average generate_time: 18.4649 seconds
#   Average prefill_time: 3.5531 seconds
#   Average decode_time: 14.9118 seconds
#   Average wait_scheduled_time: 165.5493 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 232.8921 seconds
#   Average generate_time: 20.7339 seconds
#   Average prefill_time: 2.0145 seconds
#   Average decode_time: 18.7194 seconds
#   Average wait_scheduled_time: 212.1582 seconds


# 1
# --- Stage 1 finished in 1.6973 seconds. ---
# --- Stage 2 finished in 3.7658 seconds. ---
# --- Forward Average Times (Batched) ---
#   Average end2end_time: 1.6515 seconds
#   Average generate_time: 1.6317 seconds
#   Average prefill_time: 1.1104 seconds
#   Average decode_time: 0.5214 seconds
#   Average wait_scheduled_time: 0.0198 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 3.7138 seconds
#   Average generate_time: 3.7035 seconds
#   Average prefill_time: 0.2694 seconds
#   Average decode_time: 3.4341 seconds
#   Average wait_scheduled_time: 0.0104 seconds

# 2
# --- Forward Average Times (Batched) ---
#   Average end2end_time: 2.0607 seconds
#   Average generate_time: 2.0438 seconds
#   Average prefill_time: 1.6749 seconds
#   Average decode_time: 0.3690 seconds
#   Average wait_scheduled_time: 0.0169 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 4.3086 seconds
#   Average generate_time: 4.2940 seconds
#   Average prefill_time: 0.4938 seconds
#   Average decode_time: 3.8002 seconds
#   Average wait_scheduled_time: 0.0145 seconds

# 3
# --- Stage 1 finished in 3.2989 seconds. ---
# --- Stage 2 finished in 4.9588 seconds. ---
# --- Forward Average Times (Batched) ---
#   Average end2end_time: 3.0123 seconds
#   Average generate_time: 2.9898 seconds
#   Average prefill_time: 2.6862 seconds
#   Average decode_time: 0.3036 seconds
#   Average wait_scheduled_time: 0.0224 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 4.3431 seconds
#   Average generate_time: 4.3270 seconds
#   Average prefill_time: 0.7342 seconds
#   Average decode_time: 3.5928 seconds
#   Average wait_scheduled_time: 0.0161 seconds

# 5
# --- Forward Average Times (Batched) ---
#   Average end2end_time: 5.3266 seconds
#   Average generate_time: 5.2878 seconds
#   Average prefill_time: 4.6834 seconds
#   Average decode_time: 0.6043 seconds
#   Average wait_scheduled_time: 0.0389 seconds

# --- Feedback Average Times (Batched) ---
#   Average end2end_time: 3.8419 seconds
#   Average generate_time: 3.8184 seconds
#   Average prefill_time: 1.1977 seconds
#   Average decode_time: 2.6208 seconds
#   Average wait_scheduled_time: 0.0234 seconds